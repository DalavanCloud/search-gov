Pig "session based" related queries

* compute top 25k queries in logs
* find contiguous sessions in AOL logs (basic approach = same anonID, date)
* compute co-occurence of queries within sessions, limited to top 25k queries
* group pairs of queries to form co-occurence, emit counts
* compute TFIDF on co-occurence for top 25k
* for each query in top 25k, sort by TFIDF desc, return top 25 "related queries"

Pig "click url based" related queries

* repeat top 25k selection used in session approach
* group queries by click url
* compute TFIDF score for co-occurence based on click url

------------
DONE:

* remove double quotes from queries
* omit queries with empty strings
* remove single letter queries: "l", "a", etc, require len > 1
* Use wikipedia redirect based tweet entity ngram tagger from spatialanalytics github project. Wikipedia redirects have errors, so only remap phrases if edit distance is close. 
* map city, state variants to canonical form...
* apply TFIDF pig code
* correct for frequency in TFIDF related search ranking
* Decide on good "N" for top N cutoff of session based related searches vs. Solr based related searches
--- preliminary value = top 50K searches


--------------------------
TODO:

* try restricting filtered_pairs to pairs that occur at least 30 times before running TFIDF (current threshold is 5)
* try association metric instead of TFIDF: Cxy/(Cx+Cy-Cxy)
* quick edit distance "filtered_pairs" post-filter ---> collapse filtered pairs based on string edit distance. Run pairwise edit distance and collapse closely matching strings to same entities. 
* remove domain regex matches: www.uspto.gov/web/offices/com/sol/og/ , site:www.cpc.ncep.noaa.gov, site:www.emc.ncep.noaa.govm site:www.hpc.ncep.noaa.gov, site:http://www.uspto.gov/go/classification, www.yahoo.com, site:www.swpc.noaa.gov, %2bwww.wrh.noaa.gov, www.hotmail.com, www.blumadvisory.com
* remove: enter search term(s)
* display_strings: remap lowercase output to most common mixed case form
* quick session based spelling correction fix:  if queries within the same session are within 2 characters of eachother based on edit distance, only keep the last version of the query
* collapse spelling variants with spell correct UDF
* remove known spammers/bots: seo ardan michael blum inlinks to www.blumadvisory.com
* Change ClickUrl name in pig scripts to Affiliate
* Frequency counts of queries by AnonId
* Frequency counts of queries by Date
* Frequency counts of queries by Affiliate
* Top Queries by affiliate
* instead of removing large sessions, downsample, by replacing this line: sessions = FILTER sessions BY (count >1) AND (count <=500);
* Form matrix of AnonId x Queries , where Anonid has > 1 query.  Run NMF or SVD on Matrix,
  use basis vectors to expand number of queries in user sessions.  Recompute related searches.
* look at simple query chain, sequential queries from global session frequency.. most often after obama, people search for X.. just emit sequential query pairs based on user in sliding window over ordered user history.  logical break due to activity / session change will wash out.  Alternatively can use sliding time window.
* basic topic related queries based on wikipedia link graph for entity exact matches (with redirect fixes)  Barack obama, nancy pelosi etc.  Use example in "grouping related trends" blog post.  

----------------------------


Running Pig jobs on EC2

* Starting a Cluster:
TODO: fill this in

Data Description:

hadoop@domU-12-31-39-0F-74-41:~$ hadoop fs -ls 's3://usasearch-logs/'
Found 7 items
-rwxrwxrwx   1   50490252 2010-06-16 02:42 /clicksExtract.bz2
-rwxrwxrwx   1    5659996 2010-06-16 02:42 /clicksSample
drwxrwxrwx   -          0 1970-01-01 00:00 /queries_extract_pii
-rwxrwxrwx   1 1628805636 2010-06-16 02:50 /queries_extract_pii_20100421.bz2
-rwxrwxrwx   1    1000577 2010-06-16 02:42 /queries_extract_pii_sample

queries_extract_pii_20100421.bz2 contains full anonymized PII stripped query logs from usasearch.gov and affiliates over the last 2 years (as of 4/21/2010)

Entries have the following format
(query:chararray, user_hash:chararray, timestamp:chararray, affiliate:chararray, locale:chararray, agent:chararray, is_bot:chararray);

locale, agent, and is_bot are all NULL in the current extract and can be ignored for now.


